{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f4836c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5622752",
   "metadata": {},
   "source": [
    "##### 글로럿 -> 활성화 함수 없음, 하이퍼볼릭 탄젠트, 로지스틱, 소프트맥스\n",
    "##### HE -> ReLU\n",
    "##### 르쿤 -> SELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1ca7611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.dense.Dense at 0x24ce223add0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 케라스는 균등분포의 글로럿 초기화 사용\n",
    "# He초기화 : fan_in -> ReLU\n",
    "keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal') # he_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c5508d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.dense.Dense at 0x24ceec6c8d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fan_in 대신 fan_avg기반의 균등분포 He 초기화 -> Variance Scaling\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale = 2., mode='fan_avg',\n",
    "                                                distribution='uniform')\n",
    "keras.layers.Dense(10, activation='sigmoid', kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30bef515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeakyReLU : alpha=0.01 ->  적용하려는 층 뒤에 추가\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7867cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PReLU -> 적용하려는 층 뒤에 추가\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(), # LeakyReLU(alpha=0.2)\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50027167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.dense.Dense at 0x24cefb4c110>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# elu\n",
    "keras.layers.Dense(10, activation=\"elu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb03b003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.dense.Dense at 0x24cefb48a10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selu : 모두 완전 연결 층이고, 모든 은닉층이 SELU만 쓰면 자기정규화(평균0, 표준편차1)가 된다.\n",
    "keras.layers.Dense(10, activation=\"selu\",\n",
    "                   kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0e2c3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 정규화 : 입력을 원점에 맞추고 정규화한 후, 각 층에서 두개의 새 파라미터로 결과값 스케일 조정하고 이동\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# 3136 = 784 * 4 (감마, 베타, 뮤, 시그마)\n",
    "# Non-trainable : 2368 = (3136+1200+400)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4f06f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 784)               3136      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 300)               1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 100)               400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271346 (1.04 MB)\n",
      "Trainable params: 268978 (1.03 MB)\n",
      "Non-trainable params: 2368 (9.25 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "083256ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "182fe159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활성화 함수 전에 배치 정규화 층 추가하는 것이 좋음\n",
    "# 은닉층에서 활성화함수 지정하지 않고, 배치 정규화 층 뒤에 별도의 층으로 추가해야\n",
    "# 배치 정규화 층은 입력마다 이동 파라미터 포함하기에 이전 층에서 편향 뺌\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, kernel_initializer='he_normal', use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('elu'),\n",
    "    keras.layers.Dense(100, kernel_initializer='he_normal', use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('elu'), \n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc086f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_3 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 784)               3136      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 300)               235200    \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 300)               1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation (Activation)     (None, 300)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 100)               30000     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 270546 (1.03 MB)\n",
      "Trainable params: 268378 (1.02 MB)\n",
      "Non-trainable params: 2168 (8.47 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc29010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그레이디언트 클리핑 : 역전파될 때 일정 임곗값 넘어서지 못하게 그레이디언트 잘라냄\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0) # clipnorm(그레이디언트 벡터 방향 바꾸지 못함)\n",
    "model.compile(loss='mse', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb499c9",
   "metadata": {},
   "source": [
    "전이 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f96a611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e75197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b52e9be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "# model_B_on_A를 훈련할 때 model_A도 영향 받음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91373081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영향받지 않으려면, 층 재사용하기 전에 model_A 클론하기\n",
    "model_A_clone = keras.models.clone_model(model_A) # 구조 복제\n",
    "model_A_clone.set_weights(model_A.get_weights()) # 가중치 복사(clone_model은 가중치 복사하지 않아 따로 해줘야)\n",
    "\n",
    "model_B_on_A = keras.models.Sequential(model_A_clone.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6b30571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 출력층이 랜덤하게 초기화 -> 큰 오차 -> 큰 오차 그레이디언트가 재사용된 가중치 망침\n",
    "# 처음 몇번의 에포크 동안 재사용된 층 동결하고, 새로운 층에게 적절한 가중치를 학습할 시간을 줌\n",
    "for layer in model_B_on_A.layers[:-1] :\n",
    "    layer.trainable=False  # 모든 레이어의 가중치가 훈련 불가능 -> 업데이트 안됨\n",
    "    \n",
    "# 층을 동결하거나 해제하면 다시 컴파일 해야함\n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc827147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 1s 47ms/step - loss: 1.0223 - accuracy: 0.3650 - val_loss: 0.9017 - val_accuracy: 0.3915\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.8608 - accuracy: 0.4050 - val_loss: 0.7831 - val_accuracy: 0.4899\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.7636 - accuracy: 0.4900 - val_loss: 0.7072 - val_accuracy: 0.5862\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.6994 - accuracy: 0.5600 - val_loss: 0.6603 - val_accuracy: 0.6481\n",
      "Epoch 1/16\n",
      "7/7 [==============================] - 1s 41ms/step - loss: 0.3864 - accuracy: 0.8050 - val_loss: 0.1678 - val_accuracy: 0.9726\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0963 - accuracy: 0.9950 - val_loss: 0.0980 - val_accuracy: 0.9797\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0599 - accuracy: 0.9950 - val_loss: 0.0735 - val_accuracy: 0.9868\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0427 - accuracy: 0.9950 - val_loss: 0.0777 - val_accuracy: 0.9797\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0338 - accuracy: 1.0000 - val_loss: 0.0564 - val_accuracy: 0.9868\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0244 - accuracy: 1.0000 - val_loss: 0.0507 - val_accuracy: 0.9878\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0202 - accuracy: 1.0000 - val_loss: 0.0477 - val_accuracy: 0.9878\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.0455 - val_accuracy: 0.9878\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.0475 - val_accuracy: 0.9858\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.0437 - val_accuracy: 0.9868\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.0413 - val_accuracy: 0.9878\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.0401 - val_accuracy: 0.9878\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.0394 - val_accuracy: 0.9878\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0382 - val_accuracy: 0.9888\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.0377 - val_accuracy: 0.9888\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0372 - val_accuracy: 0.9888\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1] :\n",
    "    layer.trainable = True\n",
    "    \n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a86690e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0246 - accuracy: 0.9930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.024575011804699898, 0.9929999709129333]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B) # 전이학습이 아닌 처음부터 직접짠 코드로(재사용하지 않은 층)하는 것보다 좋은 성능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ad558",
   "metadata": {},
   "source": [
    "옵티마이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93f127e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모멘텀 최적화 : 지역 최적점을 건너뛰도록 한다\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d893861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네스테로프 가속 경사 : 비용함수의 그레이디언트, 모멘텀의 올바른 방향으로\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d8a163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaGrad : 가장 가파른 차원을 따라 그레이디언트 벡터의 스케일 감소\n",
    "# 학습률 감소시키지만, 가파른 차원에 더 빠르게 감소(적응적 학습률)\n",
    "# 학습률 튜닝하지 않아도 됨\n",
    "# 너무 빨리느려져 전역 최적점에 수렴하지 못함, 너무 일찍 멈춰 심층 신경망에는 사용 X\n",
    "optimizer = keras.optimizers.Adagrad(learning_rate=0.001) # default lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a33322f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSProp : 가장 최근 반복에서 비롯된 그레이디언트만 누적\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9) # default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cce655e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam(적응적 모멘트 추정) : 모멘텀 + RMSprop\n",
    "# 학습률 튜닝하지 않아도 됨\n",
    "# Adamax, Nadam\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e79a9f3",
   "metadata": {},
   "source": [
    "#### 학습률 스케줄링  \n",
    ": 큰 학습률로 시작해 학습 속도가 느려질 때 학습률 낮추는 전략"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "572b400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 거듭제곱 기반 스케줄링 : 갈수록 천천히 감소시킴\n",
    "# lr = lr0 / (1 + steps / s)**c  (keras에서 c=1, s=1/decay)\n",
    "# ex) 학습률/2 -> 학습률/3 -> 학습률/4 -> ...\n",
    "optimizer = keras.optimizers.legacy.SGD(learning_rate=0.01, decay=1e-4) # step의 역수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac6c588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지수기반 스케줄링 : 스텝마다 10배씩 줄어듦\n",
    "# lr = lr0 * 0.1**(epoch / s)\n",
    "def exponential_decay_fn(epoch) :\n",
    "    return 0.01 * 0.1**(epoch/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "edbc21b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s) :  # 초기 학습률\n",
    "    def exponential_decay_fn(epoch) :\n",
    "        return lr0 * 0.1**(epoch/s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "# 에포크 시작할 때마다 학습률 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f79c7081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch, lr) : # 현재 학습률\n",
    "    return lr * 0.1**(1/20)\n",
    "# 초기 학습률에만 의존"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1d79bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구간별 고정 스케줄링 : 일정 횟수 에포크동안 일정한 학습률, 그다음 또 다른 횟수의 에포크 동안 작은 학습률\n",
    "def piecewise_constant_fn(epoch) :\n",
    "    if epoch < 5 :\n",
    "        return 0.01\n",
    "    elif epoch < 15 :\n",
    "        return 0.005\n",
    "    else : \n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ee71ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant(boundaries, values) :\n",
    "    boundaries = np.array([0] + boundaries)  # [0, 5, 15]\n",
    "    values = np.array(values)\n",
    "    def piecewise_constant_fn(epoch) :\n",
    "        return values[np.argmax(boundaries > epoch)-1]\n",
    "    return piecewise_constant_fn\n",
    "\n",
    "piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dfe9055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능기반 스케줄링 : 매 스텝마다 검증오차 측정하고, 오차 줄지 않으면 람다배만큼 학습률 감소시킴\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "# 최상의 검증 손실이 다섯 번의 연속적인 에포크 동안 향상되지 않을 때마다 학습률에 0.5 곱함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e19e040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras : 위들 중 하나를 사용해 학습률 정의하고, 이 학습률을 옵티마이저에 전달\n",
    "# 에포크가 아니라 매 스텝마다 학습률 업데이트\n",
    "s = 20 * len(X_train) // 32  # 20번 에포크에 담긴 전체 스텝 수\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "optimizer = keras.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ae32f",
   "metadata": {},
   "source": [
    "규제를 사용해 과대적합 피하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aedd3a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1, l2 규제\n",
    "# l2() : 각 스텝에서 호출되는 규제 객체 반환하고, 이 손실은 최종손실에 합산\n",
    "layer = keras.layers.Dense(100, activation='elu',\n",
    "                          kernel_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "# l1 : keras.regularizers.l1()\n",
    "# l1 + l2 : keras.regularizers.l1_l2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d7d561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네트워크 모든 은닉층은 동일한 활성화함수/초기화전략/규제 적용하기에 동일한 매개변수 값 반복\n",
    "# 이는 코드를 읽기 어렵게 만들고 버그 만듬 -> 반복문 사용하도록 리팩터링함\n",
    "from functools import partial  # 기본 매개변수 값 사용해 함수 호출 감쌈\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                          activation='elu',\n",
    "                          kernel_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation='softmax', kernel_initializer='glorot_uniform')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cbea02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout : 모든 입력 뉴런에 주의를 기울여야하므로 입력값의 작은 변화에 덜 민감해짐\n",
    "# 보통 출력층을 제외한 맨 위의 층부터 세 번째 층까지에 적용\n",
    "# 훈련 후 각 입력의 연결 가중치에 보존 확률(1-p) 곱해야한다.\n",
    "# 훈련 후 드롭아웃 빼고 훈련 손실 평가해야함\n",
    "# 과대적합 -> 드롭아웃 비율 늘림\n",
    "# 층 크면 -> 드롭아웃 비율 늘림\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c01fc9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 알파 드롭아웃 : SELU 활성화 함수 기반으로 자기 정규화하는 네트워크 규제\n",
    "# 입력의 평균과 표준편차 유지\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e983a710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.6718 - accuracy: 0.7569 - val_loss: 0.6327 - val_accuracy: 0.8404\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 9s 6ms/step - loss: 0.5581 - accuracy: 0.7935 - val_loss: 0.5859 - val_accuracy: 0.8450\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.5281 - accuracy: 0.8044 - val_loss: 0.5407 - val_accuracy: 0.8504\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.5080 - accuracy: 0.8115 - val_loss: 0.4888 - val_accuracy: 0.8516\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.4959 - accuracy: 0.8174 - val_loss: 0.4783 - val_accuracy: 0.8530\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4782 - accuracy: 0.8242 - val_loss: 0.4757 - val_accuracy: 0.8518\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4736 - accuracy: 0.8255 - val_loss: 0.4538 - val_accuracy: 0.8582\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.4656 - accuracy: 0.8276 - val_loss: 0.4392 - val_accuracy: 0.8656\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.4568 - accuracy: 0.8307 - val_loss: 0.4812 - val_accuracy: 0.8642\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.4557 - accuracy: 0.8319 - val_loss: 0.4490 - val_accuracy: 0.8682\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.4472 - accuracy: 0.8340 - val_loss: 0.4230 - val_accuracy: 0.8698\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.4405 - accuracy: 0.8360 - val_loss: 0.4700 - val_accuracy: 0.8704\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4367 - accuracy: 0.8369 - val_loss: 0.4301 - val_accuracy: 0.8768\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4343 - accuracy: 0.8389 - val_loss: 0.4351 - val_accuracy: 0.8752\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.4338 - accuracy: 0.8400 - val_loss: 0.4425 - val_accuracy: 0.8680\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4301 - accuracy: 0.8403 - val_loss: 0.4310 - val_accuracy: 0.8734\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.4259 - accuracy: 0.8425 - val_loss: 0.4094 - val_accuracy: 0.8760\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4206 - accuracy: 0.8432 - val_loss: 0.4298 - val_accuracy: 0.8774\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.4226 - accuracy: 0.8438 - val_loss: 0.4336 - val_accuracy: 0.8772\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.4189 - accuracy: 0.8428 - val_loss: 0.4089 - val_accuracy: 0.8816\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 20\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "226e2182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 몬테 카를로 드롭아웃 : 훈련된 모델을 재훈련하거나 전혀 수정하지 않고 성능 향상 가능\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds\n",
    "\n",
    "y_probas = np.stack([model(X_test_scaled, training=True) for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "# model(X) : model.predict(X)와 비슷하지만 텐서로 반환\n",
    "# training=True : 드롭아웃 층 활성화\n",
    "# X_test_scaled.shape : (10000, 28, 28)\n",
    "# y_probas.shape : (100, 10000, 10) ; 테스트 세트 10000개 샘플과 10개의 클래스, 모델 호출100번"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "356ce7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 120ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict(X_test_scaled[:1]), 2)  # 드롭아웃 끄고 첫 번째 샘플의 모델 예측 => 9클래스에 속함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b85380b4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.73, 0.  , 0.26]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.17, 0.  , 0.81]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.48, 0.  , 0.46]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.12, 0.  , 0.87]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.13, 0.  , 0.08, 0.  , 0.79]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.35, 0.  , 0.64]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.19, 0.  , 0.57, 0.  , 0.25]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.65, 0.  , 0.35]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.6 , 0.  , 0.39]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.23, 0.  , 0.77]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.16, 0.  , 0.78]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.42, 0.  , 0.56]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.15, 0.  , 0.85]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.21, 0.  , 0.79]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.81, 0.  , 0.16]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.21, 0.  , 0.79]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.35, 0.  , 0.59]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.9 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.15, 0.  , 0.85]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.18, 0.  , 0.2 , 0.  , 0.63]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.35, 0.  , 0.54]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.06, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.01, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.77, 0.  , 0.23]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.28, 0.  , 0.13, 0.  , 0.59]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.03, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.29, 0.  , 0.64]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.86, 0.  , 0.13]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.33, 0.  , 0.66]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.38, 0.  , 0.62]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.12, 0.  , 0.87]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.69, 0.  , 0.29]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.13, 0.  , 0.13, 0.  , 0.74]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.8 , 0.  , 0.2 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.35, 0.  , 0.6 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.93]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.78, 0.  , 0.19]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.04, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.01, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.1 , 0.  , 0.89]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.18, 0.  , 0.82]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.54, 0.  , 0.36]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.01, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.06, 0.  , 0.89]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.09, 0.  , 0.87]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.1 , 0.  , 0.89]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.07, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.06, 0.  , 0.9 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.93]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.55, 0.  , 0.4 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.12, 0.  , 0.86]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.36, 0.  , 0.48]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.36, 0.  , 0.59]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.2 , 0.  , 0.8 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.73, 0.  , 0.04, 0.  , 0.23]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.19, 0.  , 0.75]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.63, 0.  , 0.36]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.43, 0.  , 0.49]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.04, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.03, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.39, 0.  , 0.59]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.15, 0.  , 0.85]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.79, 0.  , 0.18]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.15, 0.  , 0.37, 0.  , 0.48]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.14, 0.  , 0.84]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.41, 0.  , 0.57]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.35, 0.  , 0.63]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.12, 0.  , 0.87]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.48, 0.  , 0.48]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.23, 0.  , 0.77]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.82, 0.  , 0.06]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.68, 0.  , 0.21]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.57, 0.  , 0.08, 0.  , 0.35]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.34, 0.  , 0.64]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.07, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.51, 0.  , 0.39]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.44, 0.  , 0.52]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.87, 0.  , 0.13]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.23, 0.  , 0.76]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.13, 0.  , 0.87]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.32, 0.  , 0.64]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.03, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.2 , 0.  , 0.76]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.41, 0.  , 0.56]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.12, 0.  , 0.77]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.25, 0.  , 0.74]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.92, 0.  , 0.04]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.89, 0.  , 0.11]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_probas[:, :1], 2) # 드롭아웃 활성화한 첫 번째 샘플의 모델 예측 => 9를 선호하지만 5나 7도 약간 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c667fe2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.28, 0.  , 0.67]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_proba[:1], 2) # MC드롭아웃 => 9로 예측하지만, 확신 확률 낮음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d0a5f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.26, 0.  , 0.27]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_std = y_probas.std(axis=0) # 표준 분포\n",
    "np.round(y_std[:1], 2)\n",
    "\n",
    "# 많은 분산 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26aa96f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8647"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60d0e236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.dense.Dense at 0x23105eeef90>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 맥스-노름 규제\n",
    "keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal',\n",
    "                  kernel_constraint = keras.constraints.max_norm(1.)) # 가중치 노름이 특정 값 이하가 되도록"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a6a8f6f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b36529fc",
   "metadata": {},
   "source": [
    "# 연습문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b48bb27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8) CIFAR10 이미지 데이터셋에 심층신경망 훈련하기\n",
    "# a. 100개 뉴런을 가진 은닉층 20개로 심층 신경망 만들기 (He초기화와 ELU 활성화 함수 사용)\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32,32,3]))\n",
    "for _ in range(20) :\n",
    "    model.add(keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab4f2867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Nadam 옵티마이저, 조기종료 사용해 훈련하기 \n",
    "# 이 데이터셋은 10개의 클래스와 32X32 크기의 컬러이미지 60000개로 구성(훈련 50000)\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc6362f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=keras.optimizers.Nadam(learning_rate=5e-5)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc3c87c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 315s 2us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d853459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 4.9122 - accuracy: 0.1482"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\knuyh\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 55s 21ms/step - loss: 4.9098 - accuracy: 0.1482 - val_loss: 2.1785 - val_accuracy: 0.2088\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 26s 19ms/step - loss: 2.0863 - accuracy: 0.2307 - val_loss: 1.9896 - val_accuracy: 0.2694\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 1.9681 - accuracy: 0.2765 - val_loss: 1.9093 - val_accuracy: 0.2998\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 26s 19ms/step - loss: 1.8906 - accuracy: 0.3050 - val_loss: 1.8274 - val_accuracy: 0.3372\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.8334 - accuracy: 0.3335 - val_loss: 1.7956 - val_accuracy: 0.3446\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.7832 - accuracy: 0.3508 - val_loss: 1.7916 - val_accuracy: 0.3502\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.7401 - accuracy: 0.3709 - val_loss: 1.7150 - val_accuracy: 0.3716\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.7025 - accuracy: 0.3828 - val_loss: 1.6976 - val_accuracy: 0.3870\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.6696 - accuracy: 0.3972 - val_loss: 1.7133 - val_accuracy: 0.3796\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.6373 - accuracy: 0.4103 - val_loss: 1.6337 - val_accuracy: 0.3990\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.6090 - accuracy: 0.4224 - val_loss: 1.6789 - val_accuracy: 0.3946\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.5903 - accuracy: 0.4284 - val_loss: 1.6598 - val_accuracy: 0.4114\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.5652 - accuracy: 0.4390 - val_loss: 1.6299 - val_accuracy: 0.4146\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.5483 - accuracy: 0.4453 - val_loss: 1.6238 - val_accuracy: 0.4114\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.5313 - accuracy: 0.4497 - val_loss: 1.6485 - val_accuracy: 0.4088\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.5176 - accuracy: 0.4561 - val_loss: 1.5668 - val_accuracy: 0.4358\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.4994 - accuracy: 0.4624 - val_loss: 1.5894 - val_accuracy: 0.4304\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 26s 19ms/step - loss: 1.4887 - accuracy: 0.4644 - val_loss: 1.5668 - val_accuracy: 0.4324\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.4747 - accuracy: 0.4694 - val_loss: 1.5601 - val_accuracy: 0.4396\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.4587 - accuracy: 0.4747 - val_loss: 1.5495 - val_accuracy: 0.4418\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.4473 - accuracy: 0.4798 - val_loss: 1.5698 - val_accuracy: 0.4392\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.4360 - accuracy: 0.4851 - val_loss: 1.5254 - val_accuracy: 0.4564\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.4226 - accuracy: 0.4918 - val_loss: 1.5405 - val_accuracy: 0.4474\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.4113 - accuracy: 0.4932 - val_loss: 1.5970 - val_accuracy: 0.4210\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.4023 - accuracy: 0.4992 - val_loss: 1.5287 - val_accuracy: 0.4508\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.3875 - accuracy: 0.5025 - val_loss: 1.5467 - val_accuracy: 0.4472\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.3801 - accuracy: 0.5047 - val_loss: 1.5329 - val_accuracy: 0.4570\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.3676 - accuracy: 0.5092 - val_loss: 1.5031 - val_accuracy: 0.4628\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.3569 - accuracy: 0.5135 - val_loss: 1.5611 - val_accuracy: 0.4476\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.3482 - accuracy: 0.5145 - val_loss: 1.4998 - val_accuracy: 0.4678\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.3387 - accuracy: 0.5197 - val_loss: 1.5270 - val_accuracy: 0.4606\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3278 - accuracy: 0.5223 - val_loss: 1.5226 - val_accuracy: 0.4638\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3190 - accuracy: 0.5256 - val_loss: 1.5107 - val_accuracy: 0.4612\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.3084 - accuracy: 0.5300 - val_loss: 1.5611 - val_accuracy: 0.4560\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.3024 - accuracy: 0.5306 - val_loss: 1.5400 - val_accuracy: 0.4606\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2923 - accuracy: 0.5359 - val_loss: 1.5039 - val_accuracy: 0.4694\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2864 - accuracy: 0.5376 - val_loss: 1.5107 - val_accuracy: 0.4706\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2746 - accuracy: 0.5408 - val_loss: 1.5123 - val_accuracy: 0.4698\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 25s 17ms/step - loss: 1.2670 - accuracy: 0.5457 - val_loss: 1.5300 - val_accuracy: 0.4632\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2590 - accuracy: 0.5479 - val_loss: 1.5088 - val_accuracy: 0.4696\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2538 - accuracy: 0.5486 - val_loss: 1.5105 - val_accuracy: 0.4740\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2407 - accuracy: 0.5518 - val_loss: 1.5185 - val_accuracy: 0.4784\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2355 - accuracy: 0.5581 - val_loss: 1.5440 - val_accuracy: 0.4676\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2261 - accuracy: 0.5587 - val_loss: 1.5437 - val_accuracy: 0.4626\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2193 - accuracy: 0.5641 - val_loss: 1.5706 - val_accuracy: 0.4590\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2114 - accuracy: 0.5632 - val_loss: 1.5646 - val_accuracy: 0.4616\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2047 - accuracy: 0.5666 - val_loss: 1.5606 - val_accuracy: 0.4722\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.1926 - accuracy: 0.5700 - val_loss: 1.5417 - val_accuracy: 0.4782\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.1889 - accuracy: 0.5731 - val_loss: 1.5646 - val_accuracy: 0.4642\n",
      "Epoch 50/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.1798 - accuracy: 0.5763 - val_loss: 1.5824 - val_accuracy: 0.4602\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 1.5824 - accuracy: 0.4602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5824220180511475, 0.4602000117301941]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model.h5\", save_best_only=True)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks = [early_stopping_cb, model_checkpoint_cb])\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65bae626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 55s 20ms/step - loss: 1.8394 - accuracy: 0.3400 - val_loss: 1.6269 - val_accuracy: 0.4188\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 26s 19ms/step - loss: 1.6607 - accuracy: 0.4075 - val_loss: 1.5416 - val_accuracy: 0.4516\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.5914 - accuracy: 0.4322 - val_loss: 1.5061 - val_accuracy: 0.4690\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.5451 - accuracy: 0.4519 - val_loss: 1.4981 - val_accuracy: 0.4588\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.4999 - accuracy: 0.4653 - val_loss: 1.4490 - val_accuracy: 0.4878\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.4639 - accuracy: 0.4813 - val_loss: 1.4111 - val_accuracy: 0.5032\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.4300 - accuracy: 0.4923 - val_loss: 1.4055 - val_accuracy: 0.4982\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.3995 - accuracy: 0.5021 - val_loss: 1.3922 - val_accuracy: 0.5034\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.3809 - accuracy: 0.5106 - val_loss: 1.3706 - val_accuracy: 0.5122\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3532 - accuracy: 0.5225 - val_loss: 1.3901 - val_accuracy: 0.5166\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 36s 25ms/step - loss: 1.3353 - accuracy: 0.5290 - val_loss: 1.3623 - val_accuracy: 0.5186\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 45s 32ms/step - loss: 1.3203 - accuracy: 0.5318 - val_loss: 1.3448 - val_accuracy: 0.5206\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2964 - accuracy: 0.5436 - val_loss: 1.3642 - val_accuracy: 0.5176\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.2776 - accuracy: 0.5479 - val_loss: 1.3802 - val_accuracy: 0.5050\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 37s 26ms/step - loss: 1.2648 - accuracy: 0.5546 - val_loss: 1.3547 - val_accuracy: 0.5178\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 33s 23ms/step - loss: 1.2432 - accuracy: 0.5622 - val_loss: 1.3283 - val_accuracy: 0.5342\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 1.2342 - accuracy: 0.5654 - val_loss: 1.3491 - val_accuracy: 0.5286\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.2187 - accuracy: 0.5694 - val_loss: 1.3239 - val_accuracy: 0.5316\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.2036 - accuracy: 0.5723 - val_loss: 1.3289 - val_accuracy: 0.5382\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.1869 - accuracy: 0.5800 - val_loss: 1.3437 - val_accuracy: 0.5302\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 46s 33ms/step - loss: 1.1754 - accuracy: 0.5853 - val_loss: 1.3199 - val_accuracy: 0.5384\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 25s 17ms/step - loss: 1.1626 - accuracy: 0.5884 - val_loss: 1.3438 - val_accuracy: 0.5450\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 26s 19ms/step - loss: 1.1557 - accuracy: 0.5930 - val_loss: 1.3404 - val_accuracy: 0.5340\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 33s 23ms/step - loss: 1.1400 - accuracy: 0.5990 - val_loss: 1.3719 - val_accuracy: 0.5166\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.1240 - accuracy: 0.6058 - val_loss: 1.3519 - val_accuracy: 0.5298\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.1163 - accuracy: 0.6064 - val_loss: 1.3628 - val_accuracy: 0.5310\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 25s 17ms/step - loss: 1.0956 - accuracy: 0.6156 - val_loss: 1.3638 - val_accuracy: 0.5326\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 1.0964 - accuracy: 0.6136 - val_loss: 1.3860 - val_accuracy: 0.5316\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 35s 25ms/step - loss: 1.0861 - accuracy: 0.6176 - val_loss: 1.3761 - val_accuracy: 0.5292\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.0739 - accuracy: 0.6191 - val_loss: 1.3752 - val_accuracy: 0.5290\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 33s 24ms/step - loss: 1.0651 - accuracy: 0.6247 - val_loss: 1.3460 - val_accuracy: 0.5408\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 1.0571 - accuracy: 0.6288 - val_loss: 1.3708 - val_accuracy: 0.5354\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0462 - accuracy: 0.6308 - val_loss: 1.3652 - val_accuracy: 0.5298\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.0335 - accuracy: 0.6361 - val_loss: 1.3534 - val_accuracy: 0.5370\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0186 - accuracy: 0.6406 - val_loss: 1.4038 - val_accuracy: 0.5366\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 33s 24ms/step - loss: 1.0145 - accuracy: 0.6435 - val_loss: 1.3484 - val_accuracy: 0.5404\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.0055 - accuracy: 0.6450 - val_loss: 1.3582 - val_accuracy: 0.5386\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 0.9933 - accuracy: 0.6496 - val_loss: 1.3560 - val_accuracy: 0.5374\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 0.9919 - accuracy: 0.6520 - val_loss: 1.3655 - val_accuracy: 0.5334\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 33s 23ms/step - loss: 0.9777 - accuracy: 0.6566 - val_loss: 1.3883 - val_accuracy: 0.5262\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 0.9750 - accuracy: 0.6584 - val_loss: 1.3781 - val_accuracy: 0.5454\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 1.3781 - accuracy: 0.5454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3780597448349, 0.5454000234603882]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c. 배치정규화 추가해 학습 곡선 비교\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32,32,3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20) :\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer='he_normal'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('elu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_bn_model.h5\", save_best_only=True)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=[early_stopping_cb, model_checkpoint_cb])\n",
    "\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a6685cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 27s 12ms/step - loss: 1.9197 - accuracy: 0.3120 - val_loss: 1.8176 - val_accuracy: 0.3574\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.7100 - accuracy: 0.3919 - val_loss: 1.6780 - val_accuracy: 0.3970\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.6130 - accuracy: 0.4306 - val_loss: 1.6874 - val_accuracy: 0.3994\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.5488 - accuracy: 0.4534 - val_loss: 1.6016 - val_accuracy: 0.4170\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.4905 - accuracy: 0.4763 - val_loss: 1.5455 - val_accuracy: 0.4480\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.4400 - accuracy: 0.4983 - val_loss: 1.5468 - val_accuracy: 0.4452\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.4019 - accuracy: 0.5085 - val_loss: 1.5957 - val_accuracy: 0.4456\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3620 - accuracy: 0.5250 - val_loss: 1.5008 - val_accuracy: 0.4772\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3295 - accuracy: 0.5400 - val_loss: 1.5477 - val_accuracy: 0.4572\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2903 - accuracy: 0.5517 - val_loss: 1.4837 - val_accuracy: 0.4938\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2616 - accuracy: 0.5655 - val_loss: 1.4809 - val_accuracy: 0.5000\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2314 - accuracy: 0.5764 - val_loss: 1.5009 - val_accuracy: 0.5012\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2083 - accuracy: 0.5860 - val_loss: 1.4781 - val_accuracy: 0.4922\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1695 - accuracy: 0.5975 - val_loss: 1.4821 - val_accuracy: 0.4982\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.1555 - accuracy: 0.6043 - val_loss: 1.4921 - val_accuracy: 0.4918\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.1282 - accuracy: 0.6142 - val_loss: 1.5006 - val_accuracy: 0.4944\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0997 - accuracy: 0.6240 - val_loss: 1.4923 - val_accuracy: 0.5140\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0805 - accuracy: 0.6298 - val_loss: 1.4983 - val_accuracy: 0.5028\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.0602 - accuracy: 0.6389 - val_loss: 1.5339 - val_accuracy: 0.5094\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.0397 - accuracy: 0.6454 - val_loss: 1.5241 - val_accuracy: 0.5020\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1150 - accuracy: 0.6290 - val_loss: 1.5443 - val_accuracy: 0.5032\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0328 - accuracy: 0.6483 - val_loss: 1.5159 - val_accuracy: 0.5166\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9831 - accuracy: 0.6651 - val_loss: 1.6652 - val_accuracy: 0.5070\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.9625 - accuracy: 0.6734 - val_loss: 1.5565 - val_accuracy: 0.5096\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 0.9509 - accuracy: 0.6781 - val_loss: 1.5986 - val_accuracy: 0.5016\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 0.9420 - accuracy: 0.6820 - val_loss: 1.5822 - val_accuracy: 0.5160\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9255 - accuracy: 0.6873 - val_loss: 1.5911 - val_accuracy: 0.5000\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 17.9809 - accuracy: 0.5941 - val_loss: 1.5609 - val_accuracy: 0.4754\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1470 - accuracy: 0.6034 - val_loss: 1.5778 - val_accuracy: 0.4916\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0728 - accuracy: 0.6288 - val_loss: 1.5570 - val_accuracy: 0.4972\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.0297 - accuracy: 0.6453 - val_loss: 1.5592 - val_accuracy: 0.5038\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0014 - accuracy: 0.6570 - val_loss: 1.5562 - val_accuracy: 0.5068\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 19s 14ms/step - loss: 0.9771 - accuracy: 0.6641 - val_loss: 1.6532 - val_accuracy: 0.4990\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 1.6532 - accuracy: 0.4990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.653214454650879, 0.49900001287460327]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d. 배치 정규화를 SELU로 바꾸기\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32,32,3]))\n",
    "for _ in range(20) :\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer='lecun_normal', activation='selu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=7e-4)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_selu_model.h5\", save_best_only=True)\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=[early_stopping_cb, model_checkpoint_cb])\n",
    "\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "634948c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 24s 11ms/step - loss: 1.8914 - accuracy: 0.3251 - val_loss: 1.7805 - val_accuracy: 0.3908\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.6550 - accuracy: 0.4130 - val_loss: 1.5948 - val_accuracy: 0.4416\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.5657 - accuracy: 0.4472 - val_loss: 1.6232 - val_accuracy: 0.4512\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.5000 - accuracy: 0.4730 - val_loss: 1.5833 - val_accuracy: 0.4582\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.4458 - accuracy: 0.4924 - val_loss: 1.5374 - val_accuracy: 0.4646\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.4057 - accuracy: 0.5066 - val_loss: 1.5912 - val_accuracy: 0.4748\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.3571 - accuracy: 0.5262 - val_loss: 1.5598 - val_accuracy: 0.4868\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.3174 - accuracy: 0.5396 - val_loss: 1.5927 - val_accuracy: 0.4846\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.2864 - accuracy: 0.5528 - val_loss: 1.5098 - val_accuracy: 0.4962\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.2491 - accuracy: 0.5653 - val_loss: 1.5597 - val_accuracy: 0.5006\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.2178 - accuracy: 0.5795 - val_loss: 1.5810 - val_accuracy: 0.5020\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1923 - accuracy: 0.5872 - val_loss: 1.5478 - val_accuracy: 0.5002\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1602 - accuracy: 0.5990 - val_loss: 1.5676 - val_accuracy: 0.5070\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.1340 - accuracy: 0.6065 - val_loss: 1.5484 - val_accuracy: 0.5020\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1036 - accuracy: 0.6189 - val_loss: 1.6149 - val_accuracy: 0.4946\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.0845 - accuracy: 0.6258 - val_loss: 1.5877 - val_accuracy: 0.4996\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0606 - accuracy: 0.6374 - val_loss: 1.6696 - val_accuracy: 0.5058\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0343 - accuracy: 0.6420 - val_loss: 1.7472 - val_accuracy: 0.4946\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0192 - accuracy: 0.6509 - val_loss: 1.7336 - val_accuracy: 0.4962\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 0.9956 - accuracy: 0.6579 - val_loss: 1.6619 - val_accuracy: 0.5076\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 0.9715 - accuracy: 0.6646 - val_loss: 1.7676 - val_accuracy: 0.5004\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 0.9517 - accuracy: 0.6742 - val_loss: 1.7741 - val_accuracy: 0.4984\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9366 - accuracy: 0.6809 - val_loss: 1.7890 - val_accuracy: 0.5056\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 0.9167 - accuracy: 0.6872 - val_loss: 1.7268 - val_accuracy: 0.5024\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 0.9042 - accuracy: 0.6931 - val_loss: 1.8242 - val_accuracy: 0.5008\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 0.8836 - accuracy: 0.7006 - val_loss: 1.8370 - val_accuracy: 0.5042\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 0.8661 - accuracy: 0.7054 - val_loss: 1.7272 - val_accuracy: 0.4982\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.8535 - accuracy: 0.7130 - val_loss: 1.8241 - val_accuracy: 0.5010\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 0.8391 - accuracy: 0.7138 - val_loss: 1.8142 - val_accuracy: 0.5028\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 1.8142 - accuracy: 0.5028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8141587972640991, 0.5027999877929688]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e. 알파 드롭아웃으로 모델에 규제 적용하고, 모델을 다시 훈련하지 않고 MC 드롭아웃으로 더 높은 정확도 얻기\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32,32,3]))\n",
    "for _ in range(20) :\n",
    "    model.add(keras.layers.Dense(100, activation='selu', kernel_initializer='lecun_normal'))\n",
    "model.add(keras.layers.AlphaDropout(0.1))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_alpha_dropout_model.h5\", save_best_only=True)\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=[early_stopping_cb, model_checkpoint_cb])\n",
    "\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "45a4b5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout) :\n",
    "    def call(self, inputs) :\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "24c27de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f06a4e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict_probas(mc_model, X, n_samples=10) :\n",
    "    y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(y_probas, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "874675e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10) :\n",
    "    y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7b75c55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 4s 3ms/step\n",
      "157/157 [==============================] - 1s 5ms/step\n",
      "157/157 [==============================] - 1s 5ms/step\n",
      "157/157 [==============================] - 1s 5ms/step\n",
      "157/157 [==============================] - 1s 4ms/step\n",
      "157/157 [==============================] - 1s 4ms/step\n",
      "157/157 [==============================] - 1s 3ms/step\n",
      "157/157 [==============================] - 1s 3ms/step\n",
      "157/157 [==============================] - 1s 3ms/step\n",
      "157/157 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.502"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
    "accuracy = np.mean(y_pred==y_valid[:, 0])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b448f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f. 1사이클 스케줄링으로 모델훈련\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32,32,3]))\n",
    "for _ in range(20) :\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer='lecun_normal', activation='selu'))\n",
    "model.add(keras.layers.AlphaDropout(0.1))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "optimizer=keras.optimizers.SGD(learning_rate=1e-3)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a04f63a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-2)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f850a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a6d12be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "352/352 [==============================] - 22s 47ms/step - loss: 2.0686 - accuracy: 0.2837 - val_loss: 1.8017 - val_accuracy: 0.3624\n",
      "Epoch 2/15\n",
      "352/352 [==============================] - 10s 29ms/step - loss: 1.7710 - accuracy: 0.3738 - val_loss: 1.6806 - val_accuracy: 0.4054\n",
      "Epoch 3/15\n",
      "352/352 [==============================] - 10s 29ms/step - loss: 1.6325 - accuracy: 0.4215 - val_loss: 1.6137 - val_accuracy: 0.4354\n",
      "Epoch 4/15\n",
      "352/352 [==============================] - 10s 28ms/step - loss: 1.5534 - accuracy: 0.4494 - val_loss: 1.6588 - val_accuracy: 0.4284\n",
      "Epoch 5/15\n",
      "352/352 [==============================] - 10s 28ms/step - loss: 1.4993 - accuracy: 0.4678 - val_loss: 1.6546 - val_accuracy: 0.4256\n",
      "Epoch 6/15\n",
      "352/352 [==============================] - 10s 28ms/step - loss: 1.4565 - accuracy: 0.4823 - val_loss: 1.6761 - val_accuracy: 0.4182\n",
      "Epoch 7/15\n",
      "352/352 [==============================] - 11s 31ms/step - loss: 1.4148 - accuracy: 0.4964 - val_loss: 1.5526 - val_accuracy: 0.4720\n",
      "Epoch 8/15\n",
      "352/352 [==============================] - 12s 33ms/step - loss: 1.3505 - accuracy: 0.5216 - val_loss: 1.5340 - val_accuracy: 0.4762\n",
      "Epoch 9/15\n",
      "352/352 [==============================] - 14s 40ms/step - loss: 1.2695 - accuracy: 0.5493 - val_loss: 1.4767 - val_accuracy: 0.4962\n",
      "Epoch 10/15\n",
      "352/352 [==============================] - 17s 49ms/step - loss: 1.2007 - accuracy: 0.5759 - val_loss: 1.5375 - val_accuracy: 0.4938\n",
      "Epoch 11/15\n",
      "352/352 [==============================] - 16s 45ms/step - loss: 1.1305 - accuracy: 0.5982 - val_loss: 1.4971 - val_accuracy: 0.5116\n",
      "Epoch 12/15\n",
      "352/352 [==============================] - 11s 32ms/step - loss: 1.0591 - accuracy: 0.6263 - val_loss: 1.4841 - val_accuracy: 0.5242\n",
      "Epoch 13/15\n",
      "352/352 [==============================] - 9s 27ms/step - loss: 0.9890 - accuracy: 0.6480 - val_loss: 1.5000 - val_accuracy: 0.5300\n",
      "Epoch 14/15\n",
      "352/352 [==============================] - 9s 26ms/step - loss: 0.9245 - accuracy: 0.6734 - val_loss: 1.5429 - val_accuracy: 0.5340\n",
      "Epoch 15/15\n",
      "352/352 [==============================] - 8s 23ms/step - loss: 0.8825 - accuracy: 0.6894 - val_loss: 1.5604 - val_accuracy: 0.5392\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "n_epochs = 15\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train_scaled) / batch_size) * n_epochs, max_rate=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb0f6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
